{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eddiebee/AI-Craft/blob/main/Prompt_tracking_with_Comet_and_Cohere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iruud8QA-hma"
      },
      "source": [
        "# Logging, Tracking, and Debugging Prompts using Comet\n",
        "\n",
        "In this section, we will demonstrate how to log, track, and debug prompt using the `comet-llm` library. `comet-llm` is an open-sourced repo managed by Comet. Please give the repo star if you have a chance and submit any feedback you have! https://github.com/comet-ml/comet-llm\n",
        "\n",
        "Let's first load all the necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install cohere comet-llm --quiet"
      ],
      "metadata": {
        "id": "cLNconIi-rhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5b0928-2264-4bc7-f7be-bb37a706b461"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WcRfRjy6-hmc"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import os\n",
        "import IPython\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import comet_llm\n",
        "import urllib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8yyTvrK-hmd"
      },
      "source": [
        "The function below helps to generate the final results from the model after calling the Cohere _chat_ API:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "COHERE_API_KEY = userdata.get(\"COHERE_TRIAL_KEY\")"
      ],
      "metadata": {
        "id": "Sh2yCDxbTdlO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(COHERE_API_KEY)"
      ],
      "metadata": {
        "id": "V8hqvjWmTXFg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EiKIWPuQ-hmd"
      },
      "outputs": [],
      "source": [
        "def get_completion(message,\n",
        "                   model=\"command-r-plus\",\n",
        "                   temperature=0,\n",
        "                   max_tokens=300):\n",
        "    response = co.chat(\n",
        "        model=model,\n",
        "        message=message,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHLJxr5B-hmd"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "The code below loads both the few-shot demonstrations and the validation dataset used for testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h1Q29ooc-hme"
      },
      "outputs": [],
      "source": [
        "# print markdown\n",
        "def print_markdown(text):\n",
        "    \"\"\"Prints text as markdown\"\"\"\n",
        "    IPython.display.display(IPython.display.Markdown(text))\n",
        "\n",
        "# load validation data from GitHub\n",
        "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/article-tags.json\")\n",
        "val_data = json.load(f)\n",
        "\n",
        "# load few shot data from GitHub\n",
        "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/few_shot.json\")\n",
        "few_shot_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkjHAZW0-hme"
      },
      "source": [
        "The following is a helper function to obtain the final predictions from the model given a prompt template (e.g., zero-shot or few-shot) and the provided input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FOPd0RiP-hme"
      },
      "outputs": [],
      "source": [
        "def get_predictions(prompt_template, inputs):\n",
        "\n",
        "    responses = []\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        preamble = prompt_template.format(input=inputs[i])\n",
        "        message = preamble\n",
        "        response = get_completion(message)\n",
        "        responses.append(response)\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sbh8Rl1-hme"
      },
      "source": [
        "### Few-Shot\n",
        "\n",
        "First, we define a few-shot template which will leverage the few-shot demonstration data loaded previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ur-R41tr-hme"
      },
      "outputs": [],
      "source": [
        "# function to define the few-shot template\n",
        "def get_few_shot_template(few_shot_prefix, few_shot_suffix, few_shot_examples):\n",
        "    return few_shot_prefix + \"\\n\\n\" + \"\\n\".join([ \"Abstract: \"+ ex[\"abstract\"] + \"\\n\" + \"Tags: \" + str(ex[\"tags\"]) + \"\\n\" for ex in few_shot_examples]) + \"\\n\\n\" + few_shot_suffix\n",
        "\n",
        "# function to sample few shot data\n",
        "def random_sample_data (data, n):\n",
        "    return np.random.choice(few_shot_data, n, replace=False)\n",
        "\n",
        "\n",
        "# the few-shot prefix and suffix\n",
        "few_shot_prefix = \"\"\"Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\"\"\"\n",
        "few_shot_suffix = \"\"\"Abstract: {input}\\nTags:\"\"\"\n",
        "\n",
        "# load 3 samples from few shot data\n",
        "few_shot_template = get_few_shot_template(few_shot_prefix, few_shot_suffix, random_sample_data(few_shot_data, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z716KoK2-hmf",
        "outputId": "6739e3f7-a7d8-4516-feeb-611de0b367d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\"model_name\"]. If you don\\'t find model names in the abstract or you are not sure, return [\"NA\"]\\n\\nAbstract: Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.\\nTags: [\\'CCNNs\\']\\n\\nAbstract: Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.\\nTags: [\\'SadTalker\\', \\'ExpNet\\', \\'PoseVAE\\']\\n\\nAbstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI). However, the expensive training and deployment of LLMs present challenges to transparent and open academic research. To address these issues, this project open-sources the Chinese LLaMA and Alpaca large models, emphasizing instruction fine-tuning. We expand the original LLaMA\\'s Chinese vocabulary by adding 20K Chinese tokens, increasing encoding efficiency and enhancing basic semantic understanding. By incorporating secondary pre-training using Chinese data and fine-tuning with Chinese instruction data, we substantially improve the models\\' comprehension and execution of instructions. Our pilot study serves as a foundation for researchers adapting LLaMA and Alpaca models to other languages. Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond. GitHub repository:\\nTags: [\\'ChatGPT\\', \\'GPT-4\\', \\'LLaMA\\', \\'Alpaca\\']\\n\\n\\nAbstract: {input}\\nTags:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "few_shot_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvvfx8ae-hmf"
      },
      "source": [
        "### Zero-Shot Template\n",
        "\n",
        "The code below defines the zero-shot template. Note that we use the same instruction from the few-shot prompt template. But in this case, we don't use the demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wYvzzwYA-hmf"
      },
      "outputs": [],
      "source": [
        "zero_shot_template = \"\"\"\n",
        "Your task is extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
        "\n",
        "Abstract: {input}\n",
        "Tags:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvsFEof-hmf"
      },
      "source": [
        "### Get Predictions\n",
        "\n",
        "We then generated all the predictions using the validation data as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XGhppRv--hmg"
      },
      "outputs": [],
      "source": [
        "# get the predictions\n",
        "\n",
        "abstracts = [val_data[i][\"abstract\"] for i in range(len(val_data))]\n",
        "few_shot_predictions = get_predictions(few_shot_template, abstracts)\n",
        "zero_shot_predictions = get_predictions(zero_shot_template, abstracts)\n",
        "expected_tags = [str(val_data[i][\"tags\"]) for i in range(len(val_data))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5to6iSU2-hmg",
        "outputId": "64883681-eaf5-40ea-f3e5-e201f8941de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few shot predictions\n",
            "['[\"CCNNs\"]\\n[\"SadTalker\", \"ExpNet\", \"PoseVAE\"]\\n[\"ChatGPT\", \"GPT-4\", \"LLaMA\", \"Alpaca\"]\\n[\"Evol-Instruct\", \"WizardLM\", \"ChatGPT\"]', '[\"CCNNs\", \"SadTalker\", \"ExpNet\", \"PoseVAE\", \"ChatGPT\", \"GPT-4\", \"LLaMA\", \"Alpaca\", \"FLAN-T5\"]', '[\"NA\"]', '[\"NA\"]', '[\"CCNNs\"]\\n[\"SadTalker\", \"ExpNet\", \"PoseVAE\"]\\n[\"ChatGPT\", \"GPT-4\", \"LLaMA\", \"Alpaca\"]\\n[\"ChatGPT\"]', '[\"ViT\"]', '[\"SadTalker\", \"ExpNet\", \"PoseVAE\", \"ChatGPT\", \"GPT-4\", \"LLaMA\", \"Alpaca\", \"Inpaint Anything (IA)\"]', '[\"Anything-3D\", \"BLIP\", \"Segment-Anything\"]', '[\"CCNNs\"]\\n[\"SadTalker\", \"ExpNet\", \"PoseVAE\"]\\n[\"ChatGPT\", \"GPT-4\", \"LLaMA\", \"Alpaca\"]\\n[\"Chameleon\"]', '[\"NA\"]']\n",
            "\n",
            "\n",
            "Zero shot predictions\n",
            "['[\"WizardLM\", \"LLaMA\", \"ChatGPT\"]', '[\"FLAN-T5\"]', '[\"NA\"]', '[\"PAXQA\"]', '[\"ChatGPT\"]', '[\"ViT\"]', '[\"Segment-Anything Model\", \"Inpaint Anything\"]', '[\"Anything-3D\", \"BLIP\", \"Segment-Anything\"]', '[\"Chameleon\"]', '[\"NA\"]']\n",
            "\n",
            "\n",
            "Expected tags\n",
            "[\"['LLaMA', 'ChatGPT', 'WizardLM']\", \"['FLAN-T5', 'FLAN']\", \"['NA']\", \"['PAXQA']\", \"['ChatGPT']\", \"['OpenCLIP', 'ViT']\", \"['SAM', 'IA']\", \"['Anything-3D', 'BLIP', 'Segment-Anything']\", \"['Chameleon', 'GPT-4', 'ChatGPT']\", \"['NA']\"]\n"
          ]
        }
      ],
      "source": [
        "print(\"Few shot predictions\")\n",
        "print(few_shot_predictions)\n",
        "print(\"\\n\\nZero shot predictions\")\n",
        "print(zero_shot_predictions)\n",
        "print(\"\\n\\nExpected tags\")\n",
        "print(expected_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq47EV18-hmg"
      },
      "source": [
        "### Log Prompt Results\n",
        "\n",
        "Finally, we log the prompt + results to Comet. Note that we are logging both the few-shot and zero-shot results, together with all the metadata and tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iBhWMBZa-hmg",
        "outputId": "252e74b3-d7cf-435f-d2f4-701720f52b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in /root/.comet.config (set COMET_CONFIG to change where it is saved).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt logged to https://www.comet.com/eddiebee/ml-paper-tagger-prompts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:comet_llm.summary:Prompt logged to https://www.comet.com/eddiebee/ml-paper-tagger-prompts\n"
          ]
        }
      ],
      "source": [
        "# log the predictions in Comet along with the ground truth for comparison\n",
        "\n",
        "# set up comet\n",
        "COMET_API_KEY = userdata.get(\"COMET_API_KEY\")\n",
        "COMET_WORKSPACE = \"eddiebee\"\n",
        "\n",
        "# initialize comet\n",
        "comet_llm.init(COMET_API_KEY,\n",
        "               COMET_WORKSPACE,\n",
        "               project=\"ml-paper-tagger-prompts\")\n",
        "\n",
        "# log the predictions\n",
        "for i in range(len(expected_tags)):\n",
        "    # log the few-shot predictions\n",
        "    comet_llm.log_prompt(\n",
        "        prompt=few_shot_template.format(input=abstracts[i]),\n",
        "        prompt_template=few_shot_template,\n",
        "        output=few_shot_predictions[i],\n",
        "        tags = [\"cohere\", \"command-r-plus\" \"few-shot\"],\n",
        "        metadata = {\n",
        "            \"expected_tags\": expected_tags[i],\n",
        "            \"abstract\": abstracts[i],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # log the zero-shot predictions\n",
        "    comet_llm.log_prompt(\n",
        "        prompt=zero_shot_template.format(input=abstracts[i]),\n",
        "        prompt_template=zero_shot_template,\n",
        "        output=zero_shot_predictions[i],\n",
        "        tags = [\"cohere\", \"command-r-plus\" \"zero-shot\"],\n",
        "        metadata = {\n",
        "            \"expected_tags\": expected_tags[i],\n",
        "            \"abstract\": abstracts[i],\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WPD8MnvYCSv"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}